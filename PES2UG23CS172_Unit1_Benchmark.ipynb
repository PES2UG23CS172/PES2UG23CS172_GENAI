{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq-HWBh5tMGA",
        "outputId": "3675af55-d388-4011-eeb0-d20773fe50f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "-efU92RgtZye"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = \"bert-base-uncased\"\n",
        "roberta_model = \"roberta-base\"\n",
        "bart_model = \"facebook/bart-base\"\n"
      ],
      "metadata": {
        "id": "XUyHt7OAvIwJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 1: TEXT GENERATION"
      ],
      "metadata": {
        "id": "xh_MibuBv0Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    gen_bert = pipeline(\"text-generation\", model=bert_model)\n",
        "    output = gen_bert(\"The future of Artificial Intelligence is\", max_length=30)\n",
        "    print(output)\n",
        "except Exception as e:\n",
        "    print(\"BERT Generation Error:\", e)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKiU63nQvUtb",
        "outputId": "b634d03f-ccaf-4369-b3d7-fd9556a214b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    gen_roberta = pipeline(\"text-generation\", model=roberta_model)\n",
        "    output = gen_roberta(\"The future of Artificial Intelligence is\", max_length=30)\n",
        "    print(output)\n",
        "except Exception as e:\n",
        "    print(\"RoBERTa Generation Error:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95AtOd9ovZlK",
        "outputId": "aaef859e-07ea-44a0-ddf4-a909b1359f1c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'The future of Artificial Intelligence is'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_bart = pipeline(\"text-generation\", model=bart_model)\n",
        "output = gen_bart(\"The future of Artificial Intelligence is\", max_length=30)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRe4RrImvfqz",
        "outputId": "77346ed5-3291-43f2-d2df-f5e10914e55a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'The future of Artificial Intelligence is sluggish communication attire attire bolt558558 incarnation Cannot STR stare continental attire attire attireerb shop scenery prejudice incarnationdest currently shone doublesschool pepp Doveigmatic currently2006bdaticigmaticbda attire Athena release Allow currently attire insertsbdabda continental overseerousrous Hotbdathing Hot overseeasuryrousrous oversee conqueringthing OD 289 consec Meccabda Zin confusion confusion confusionnerigmatic Hot consec confusionbdaMYigmatic musicians oversee childbirth overseerousigmatic Meccanerundersriott musiciansriott oversee currently Meccaundersriottendrariottigmatic confusion currentlyMYnerner musiciansigmatic currentlynerner existence overseener confusion confusionzeb confusion overseeunders viceriottner2009ner2009 EA confusionCountry overseeriottbda ballot overseener ballot confusion confusion oversee confusion confusion Meccaner Zin oversee Pantherzeb2009 currently Closener confusionCountry confusion confusionigmatic confusionbdariott confusion confusionasuryrousriott overseeriottundersCountry oversee20092009 ballot vicekernelner confusion waivers currentlybdabda Zin overseener BMI ballot Meccaner Meccariottriottriott2009nernerrousundersner currently currently currentlyrousunders confusion Mecca2009 Mecca oversee2009 trying2009riott confusion ballot ballotrous2009nerrous ballot BMI2009riott mag ballot primedriott Eisen confusionCountrynerzeb confusion Meccariott waivers overseeOffset confusion ballotriott currentlyfutureriottCountry confusion2009 skeletalnerzebnernernerCountry confusionner ballotner skeletal ballot2009erb'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 2: FILL-MASK\n"
      ],
      "metadata": {
        "id": "0pL-aSEgvvBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fill_bert = pipeline(\"fill-mask\", model=bert_model)\n",
        "\n",
        "sentence = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "results = fill_bert(sentence)\n",
        "\n",
        "for r in results:\n",
        "    print(r[\"token_str\"], r[\"score\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs49YKNkwGz0",
        "outputId": "ab26fa98-9e12-4a4a-b2f4-e3c04685a5f2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "create 0.5396888852119446\n",
            "generate 0.15575668215751648\n",
            "produce 0.054054468870162964\n",
            "develop 0.04451529309153557\n",
            "add 0.01757732406258583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_roberta = pipeline(\"fill-mask\", model=roberta_model)\n",
        "\n",
        "sentence = \"The goal of Generative AI is to <mask> new content.\"\n",
        "results = fill_roberta(sentence)\n",
        "\n",
        "for r in results:\n",
        "    print(r[\"token_str\"], r[\"score\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBoFQFhdwMAf",
        "outputId": "97c4f76c-d921-436e-cfe1-36b9bb5393a9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " generate 0.3711293935775757\n",
            " create 0.36771273612976074\n",
            " discover 0.08351442217826843\n",
            " find 0.021335095167160034\n",
            " provide 0.016521504148840904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fill_bart = pipeline(\"fill-mask\", model=bart_model)\n",
        "\n",
        "sentence = \"The goal of Generative AI is to <mask> new content.\"\n",
        "results = fill_bart(sentence)\n",
        "\n",
        "for r in results:\n",
        "    print(r[\"token_str\"], r[\"score\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vmivBPKwPRt",
        "outputId": "bfa96383-923a-47c6-dd05-d79f3af77303"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " create 0.0746147632598877\n",
            " help 0.06571780890226364\n",
            " provide 0.060879286378622055\n",
            " enable 0.03593532741069794\n",
            " improve 0.03319435939192772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERIMENT 3: QUESTION ANSWERING"
      ],
      "metadata": {
        "id": "bqAy9ua6wcXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_bert = pipeline(\"question-answering\", model=bert_model)\n",
        "\n",
        "result = qa_bert(\n",
        "    question=\"What are the risks?\",\n",
        "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        ")\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_slubEECwmDP",
        "outputId": "566573c8-5be5-41f7-9291-d4c96b34c9b1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.009028168860822916, 'start': 14, 'end': 81, 'answer': 'poses significant risks such as hallucinations, bias, and deepfakes'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_roberta = pipeline(\"question-answering\", model=roberta_model)\n",
        "\n",
        "result = qa_roberta(\n",
        "    question=\"What are the risks?\",\n",
        "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        ")\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmAKKDz6wp0K",
        "outputId": "3c91346c-0a5e-484b-b65b-c782bca678c5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.008323365822434425, 'start': 68, 'end': 81, 'answer': 'and deepfakes'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_bart = pipeline(\"question-answering\", model=bart_model)\n",
        "\n",
        "result = qa_bart(\n",
        "    question=\"What are the risks?\",\n",
        "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        ")\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHSpIYBTwsm3",
        "outputId": "8b682496-f5d5-475b-c3ae-12b5e43cedb4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.08922647871077061, 'start': 0, 'end': 10, 'answer': 'Generative'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task       | Model           | Success/Failure | Observation         | Architectural Reason |\n",
        "| ---------- | --------------- | --------------- | ------------------- | -------------------- |\n",
        "| Generation | BERT            |  Failure       | Error    | Encoder-only         |\n",
        "| Generation | RoBERTa         |  Failure       | Cannot autoregress  | Encoder-only         |\n",
        "| Generation | BART            |  Success       | Fluent continuation | Encoder-Decoder      |\n",
        "| Fill-Mask  | BERT            |  Success       | High confidence       | MLM training         |\n",
        "| Fill-Mask  | RoBERTa         |  Success       | High confidence     | Optimized MLM        |\n",
        "| Fill-Mask  | BART            |  Partial       | Less confident      | Not MLM-centric      |\n",
        "| QA         | All base models |  Weak          | Random/partial      | Not -fine-tuned |\n"
      ],
      "metadata": {
        "id": "zCtQxyq_yPXN"
      }
    }
  ]
}